{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e876ada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\mayk.barbosa\\Desktop\\Projetos\\Pesquisa\\dissertacao_final_code\\spt_models_and_simulations\\venv\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\mayk.barbosa\\Desktop\\Projetos\\Pesquisa\\dissertacao_final_code\\spt_models_and_simulations\\venv\\Lib\\site-packages\\tensorflow_probability\\python\\internal\\backend\\numpy\\_utils.py:48: The name tf.logging.TaskLevelStatusMessage is deprecated. Please use tf.compat.v1.logging.TaskLevelStatusMessage instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\mayk.barbosa\\Desktop\\Projetos\\Pesquisa\\dissertacao_final_code\\spt_models_and_simulations\\venv\\Lib\\site-packages\\tensorflow_probability\\python\\internal\\backend\\numpy\\_utils.py:48: The name tf.control_flow_v2_enabled is deprecated. Please use tf.compat.v1.control_flow_v2_enabled instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Utils\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd \n",
    "import time \n",
    "import random\n",
    "\n",
    "# Neural Nets\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "from keras.saving import register_keras_serializable\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Config \n",
    "import sys\n",
    "import os\n",
    "import pickle\n",
    "from dotenv import load_dotenv\n",
    "import warnings\n",
    "\n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.append(parent_dir)\n",
    "\n",
    "# Data\n",
    "from mkt_data_ETL.data_load_and_transform import get_data, get_top_mkt_cap_stocks\n",
    "\n",
    "load_dotenv()\n",
    "# Suppress all warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ed567db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All seeds set to: 42\n",
      "Deterministic operations enabled for reproducible results.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# REPRODUCIBILITY SETUP - Set seeds for deterministic results\n",
    "# =============================================================================\n",
    "def set_seeds(seed=42):\n",
    "    \"\"\"\n",
    "    Set seeds for reproducible results across all random number generators.\n",
    "    \n",
    "    Args:\n",
    "        seed (int): The seed value to use for all random number generators\n",
    "    \"\"\"\n",
    "    # Set Python's built-in random seed\n",
    "    random.seed(seed)\n",
    "    \n",
    "    # Set NumPy seed\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Set TensorFlow seeds\n",
    "    tf.random.set_seed(seed)\n",
    "    \n",
    "    # Set environment variables for deterministic behavior\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "    os.environ['TF_CUDNN_DETERMINISTIC'] = '1'\n",
    "    \n",
    "    # Configure TensorFlow for deterministic operations\n",
    "    tf.config.experimental.enable_op_determinism()\n",
    "    \n",
    "    print(f\"All seeds set to: {seed}\")\n",
    "    print(\"Deterministic operations enabled for reproducible results.\")\n",
    "\n",
    "# Set the seed for reproducible results\n",
    "SEED = 42\n",
    "set_seeds(SEED)\n",
    "\n",
    "def verify_seed_settings():\n",
    "    \"\"\"\n",
    "    Verify that all seed settings are properly configured.\n",
    "    This function can be called to check reproducibility setup.\n",
    "    \"\"\"\n",
    "    print(\"=== Reproducibility Verification ===\")\n",
    "    print(f\"SEED value: {SEED}\")\n",
    "    print(f\"PYTHONHASHSEED: {os.environ.get('PYTHONHASHSEED', 'Not set')}\")\n",
    "    print(f\"TF_DETERMINISTIC_OPS: {os.environ.get('TF_DETERMINISTIC_OPS', 'Not set')}\")\n",
    "    print(f\"TF_CUDNN_DETERMINISTIC: {os.environ.get('TF_CUDNN_DETERMINISTIC', 'Not set')}\")\n",
    "    print(\"=====================================\")\n",
    "\n",
    "# Uncomment the line below to verify seed settings\n",
    "# verify_seed_settings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ceccffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data \n",
    "stock_prices_df, stock_shares_amount_df, mkt_cap_df, spx_index, removed_companies = get_data()\n",
    "top_100_mkt_cap_df, top_100_mkt_cap_prices_df=get_top_mkt_cap_stocks(stock_prices_df=stock_prices_df, \n",
    "stock_mkt_cap_df=mkt_cap_df)\n",
    "\n",
    "\n",
    "# Calculate daily returns\n",
    "stocks_returns    = np.log(top_100_mkt_cap_prices_df / top_100_mkt_cap_prices_df.shift(1))\n",
    "sp500_idx_returns =  np.log(spx_index / spx_index.shift(1))\n",
    "\n",
    "# Rolling volatility\n",
    "window_size = 252*2 # 2 years\n",
    "Sigma_df= stocks_returns.rolling(window=window_size).cov(pairwise=True)\n",
    "Sigma_df = Sigma_df.dropna()\n",
    "\n",
    "# Get the first date in the cleaned DataFrame\n",
    "START_DATE = Sigma_df.index.get_level_values(0)[0]\n",
    "\n",
    "# Filter dataframes to start from START_DATE\n",
    "top_100_mkt_cap_df = top_100_mkt_cap_df.loc[START_DATE:]\n",
    "stocks_returns     = stocks_returns.loc[START_DATE:]\n",
    "stock_prices_df    = stock_prices_df.loc[START_DATE:]\n",
    "\n",
    "assets = stocks_returns.columns\n",
    "dates  = stocks_returns.index\n",
    "\n",
    "n = len(assets)\n",
    "T = len(dates)\n",
    "Sigma_t = np.empty((T, n, n))\n",
    "\n",
    "# Fill array with each rolling covariance matrix\n",
    "for i, t in enumerate(dates):\n",
    "    Sigma = Sigma_df.loc[t].reindex(index=assets, columns=assets).values\n",
    "    Sigma_t[i] = Sigma\n",
    "\n",
    "\n",
    "# Market Weights\n",
    "mkt_portfolio_weights = top_100_mkt_cap_df.div(top_100_mkt_cap_df.sum(axis=1),axis=0)\n",
    "\n",
    "# Market portfolio return\n",
    "mkt_return = (mkt_portfolio_weights.shift(1) * stocks_returns).sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f32db8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing tau_t calculation\n"
     ]
    }
   ],
   "source": [
    "def compute_relative_covariance(sigma: np.ndarray, pi: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Calcula a matriz de covariância relativa τ_ij^π(t) com base na covariância σ_ij(t)\n",
    "    e no vetor de pesos do portfólio π(t).\n",
    "    \n",
    "    Args:\n",
    "        sigma (np.ndarray): Matriz de covariância dos ativos (n x n).\n",
    "        pi (np.ndarray): Vetor de pesos do portfólio (n,).\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Matriz de covariância relativa τ^π (n x n).\n",
    "    \"\"\"\n",
    "    # Covariância ativo i com portfólio: sigma_iπ = sigma @ pi\n",
    "    sigma_i_pi = sigma @ pi        # (n,)\n",
    "    sigma_pi_pi = pi.T @ sigma @ pi  # escalar\n",
    "\n",
    "    # Matriz τ_ij^π = σ_ij - σ_iπ - σ_jπ + σ_ππ\n",
    "    n = len(pi)\n",
    "    tau = np.zeros((n, n))\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            tau[i, j] = sigma[i, j] - sigma_i_pi[i] - sigma_i_pi[j] + sigma_pi_pi\n",
    "    return tau\n",
    "\n",
    "print(\"initializing tau_t calculation\")\n",
    "start_time = time.time()\n",
    "\n",
    "tau_t = np.empty((T, n, n))\n",
    "for t in range(T):\n",
    "    tau_t[t] = compute_relative_covariance(Sigma_t[t], mkt_portfolio_weights.iloc[t].values)\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_seconds = end_time - start_time\n",
    "print(f\"Elapsed time: {elapsed_seconds:.2f} seconds\")\n",
    "\n",
    "\n",
    "# Copying data generated previously \n",
    "mu_t_df = mkt_portfolio_weights.copy()\n",
    "R_t_df =  stocks_returns.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b6b2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def G_func(mu, p=0.5):\n",
    "    S = np.sum(mu**p, axis=1)                # (T,)\n",
    "    return S**(1.0/p)                          # (T,)\n",
    "\n",
    "def grad_G(mu: np.ndarray, p: float = 0.5) -> np.ndarray: \n",
    "    S = np.sum(mu**p, axis=1)                     # (T,)\n",
    "    pref = S**(1.0/p - 1.0)                       # (T,)\n",
    "    return (mu**(p - 1.0)) * pref[:, None]        # (T, N)         \n",
    "\n",
    "def grad_log_G(mu, p=0.5):\n",
    "    S = np.sum(mu**p, axis=1)   \n",
    "    return (mu**(p - 1.0)) / S[:, None] \n",
    "\n",
    "def dwp_weights(mu: np.ndarray, p: float=0.5) -> np.ndarray:\n",
    "    num = mu**p\n",
    "    den = num.sum(axis=1, keepdims=True)\n",
    "    return num / den\n",
    "\n",
    "def dwp_simple_returns(mu: np.ndarray, r: np.ndarray, p: float = 0.5) -> np.ndarray:\n",
    "    pi = dwp_weights(mu, p)                  # (T,N)\n",
    "    w = np.empty_like(pi)\n",
    "    w[1:] = pi[:-1]                          # shift(1)\n",
    "    w[0]  = np.nan\n",
    "    return (w * r).sum(axis=1)    \n",
    "\n",
    "\n",
    "def hess_G_dwp(mu: np.ndarray, p: float = 0.5) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Hessiana de G(μ) para o Diversity-Weighted Portfolio:\n",
    "        G(μ) = (∑ μ_i^p)^(1/p).\n",
    "    Retorna tensor (T, n, n) com D_{ij}G(μ_t).\n",
    "    \"\"\"\n",
    "    T, n = mu.shape\n",
    "    S   = np.sum(mu**p, axis=1)                         # (T,)\n",
    "    Sa  = S**(1.0/p - 1.0)                              # S^{a}, a = 1/p - 1\n",
    "    # termo diagonal: (p-1) S^{a} * diag( μ^{p-2} )\n",
    "    diag_power = mu**(p - 2.0)                          # (T,n)\n",
    "    term1 = (p - 1.0) * Sa[:, None, None] * np.einsum('ti,ij->tij', diag_power, np.eye(n))\n",
    "\n",
    "    # termo de rank-1: (1-p) S^{a-1} * (μ^{p-1} ⊗ μ^{p-1})\n",
    "    v     = mu**(p - 1.0)                               # (T,n)\n",
    "    term2 = (1.0 - p) * (Sa / S)[:, None, None] * (v[:, :, None] * v[:, None, :])\n",
    "\n",
    "    return term1 + term2\n",
    "\n",
    "def drift_g(mu: np.ndarray, tau: np.ndarray, p: float = 0.5) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Calcula g_t = -(1/(2 G)) * sum_{i,j} D_{ij}G * μ_i μ_j τ_{ij}, para t=1..T.\n",
    "    Entradas:\n",
    "      - mu  : (T, n) pesos de mercado μ(t)\n",
    "      - tau : (T, n, n) covariância relativa τ^{μ}(t)\n",
    "    Saída:\n",
    "      - g   : (T,) série temporal do integrando do drift\n",
    "    \"\"\"\n",
    "    # G(μ)\n",
    "    G = (np.sum(mu**p, axis=1))**(1.0/p)                # (T,)\n",
    "\n",
    "    # Hessiana D^2 G(μ)\n",
    "    H = hess_G_dwp(mu, p=p)                             # (T, n, n)\n",
    "\n",
    "    # sum_{i,j} D_{ij}G * μ_i * μ_j * τ_{ij}  (evita construir μ⊗μ explicitamente)\n",
    "    summed = np.einsum('tij,ti,tj,tij->t', H, mu, mu, tau)   # (T,)\n",
    "\n",
    "    g = -0.5 * summed / G\n",
    "    return g \n",
    "import numpy as np\n",
    "\n",
    "def integrate_g_simpson_or_trap(dg_t: np.ndarray, dt: float = 1.0) -> float:\n",
    "    \"\"\"\n",
    "    Aproxima ∫ g(t) dt a partir de amostras dg_t[0..T-1].\n",
    "    - Se T ímpar: Simpson composto com passo dt.\n",
    "    - Se T par : Trapézio composto com passo dt.\n",
    "    Requer passo uniforme (dt escalar). \n",
    "    \"\"\"\n",
    "    dg_t = np.asarray(dg_t, dtype=float)\n",
    "    T = dg_t.shape[0]\n",
    "    if T == 1:\n",
    "        return 0.0\n",
    "\n",
    "    if T % 2 == 1:\n",
    "        # Simpson: h/3*(f0 + fT + 4*sum(f_odd) + 2*sum(f_even))\n",
    "        s_odd  = dg_t[1:-1:2].sum()   # índices 1,3,5,...\n",
    "        s_even = dg_t[2:-1:2].sum()   # índices 2,4,6,...\n",
    "        return (dt/3.0) * (dg_t[0] + dg_t[-1] + 4.0*s_odd + 2.0*s_even)\n",
    "    else:\n",
    "        # Trapézio composto: h*(0.5*f0 + sum(f_mid) + 0.5*fT)\n",
    "        return dt * (0.5*dg_t[0] + dg_t[1:-1].sum() + 0.5*dg_t[-1])\n",
    "\n",
    "def integrate_g_hybrid(dg_t: np.ndarray, dt: float = 1.0) -> float:\n",
    "    \"\"\"\n",
    "    Híbrido: se T par, aplica Simpson nos primeiros T-1 pontos e\n",
    "    trapézio no último intervalo (T-2,T-1). dt uniforme.\n",
    "    \"\"\"\n",
    "    dg_t = np.asarray(dg_t, dtype=float)\n",
    "    T = dg_t.shape[0]\n",
    "    if T <= 1:\n",
    "        return 0.0\n",
    "    if T % 2 == 1:\n",
    "        # Simpson em toda a malha\n",
    "        return integrate_g_simpson_or_trap(dg_t, dt=dt)\n",
    "    else:\n",
    "        # Simpson nos 0..T-2 (tem T-1 pontos => ímpar)\n",
    "        part_simpson = integrate_g_simpson_or_trap(dg_t[:-1], dt=dt)\n",
    "        # Trapézio no último intervalo\n",
    "        part_trap = dt * 0.5 * (dg_t[-2] + dg_t[-1])\n",
    "        return part_simpson + part_trap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bcecabf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.008008495330664167\n"
     ]
    }
   ],
   "source": [
    "p= 0.5 \n",
    "# Mercado \n",
    "mu = mu_t_df.values # Pesos mercado\n",
    "ret_mkt = np.exp(np.sum(mkt_return.values))\n",
    "\n",
    "# G_p \n",
    "G_p = G_func(mu, p=0.5)\n",
    "grad_G_p = grad_G(mu)\n",
    "grad_log_G_p = grad_log_G(mu)\n",
    "\n",
    "\n",
    "# DWP e retorno do DWP\n",
    "pi_dwp = dwp_weights(mu, p=p)\n",
    "dwp_ret = dwp_simple_returns(pi_dwp, stocks_returns.values)\n",
    "dwp_ret = np.nan_to_num(dwp_ret)\n",
    "dwp_cumulative = np.exp(np.sum(dwp_ret))\n",
    "\n",
    "\n",
    "# Master equation \n",
    "g_t   = drift_g(mu, tau_t, p=p)\n",
    "integral_g = integrate_g_hybrid(g_t, dt=1.0)\n",
    "\n",
    "\n",
    "left_hand_side = np.log(dwp_cumulative)- np.log(ret_mkt)\n",
    "right_hand_side = (np.log(G_p[-1])- np.log(G_p[0])) + integral_g\n",
    "\n",
    "\n",
    "diff_master_formula = left_hand_side - right_hand_side\n",
    "print(diff_master_formula)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea496ed3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a547d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c614a1f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f814f309",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582db226",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538aea04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3968c24d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
