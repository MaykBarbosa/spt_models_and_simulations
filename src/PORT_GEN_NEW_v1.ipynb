{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datetime import datetime\n",
    "\n",
    "import time \n",
    "#Neural Nets\n",
    "import tensorflow as tf\n",
    "\n",
    "# Add the parent directory to the Python path to allow imports from tests/\n",
    "import sys\n",
    "import os\n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.append(parent_dir)\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from mkt_data_ETL.data_load_and_transform import get_data, get_top_mkt_cap_stocks\n",
    "\n",
    "\n",
    "import warnings\n",
    "import random\n",
    "\n",
    "load_dotenv()\n",
    "# Suppress all warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# REPRODUCIBILITY SETUP - Set seeds for deterministic results\n",
    "# =============================================================================\n",
    "def set_seeds(seed=42):\n",
    "    \"\"\"\n",
    "    Set seeds for reproducible results across all random number generators.\n",
    "    \n",
    "    Args:\n",
    "        seed (int): The seed value to use for all random number generators\n",
    "    \"\"\"\n",
    "    # Set Python's built-in random seed\n",
    "    random.seed(seed)\n",
    "    \n",
    "    # Set NumPy seed\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Set TensorFlow seeds\n",
    "    tf.random.set_seed(seed)\n",
    "    \n",
    "    # Set environment variables for deterministic behavior\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "    os.environ['TF_CUDNN_DETERMINISTIC'] = '1'\n",
    "    \n",
    "    # Configure TensorFlow for deterministic operations\n",
    "    tf.config.experimental.enable_op_determinism()\n",
    "    \n",
    "    print(f\"All seeds set to: {seed}\")\n",
    "    print(\"Deterministic operations enabled for reproducible results.\")\n",
    "\n",
    "# Set the seed for reproducible results\n",
    "SEED = 42\n",
    "set_seeds(SEED)\n",
    "\n",
    "def verify_seed_settings():\n",
    "    \"\"\"\n",
    "    Verify that all seed settings are properly configured.\n",
    "    This function can be called to check reproducibility setup.\n",
    "    \"\"\"\n",
    "    print(\"=== Reproducibility Verification ===\")\n",
    "    print(f\"SEED value: {SEED}\")\n",
    "    print(f\"PYTHONHASHSEED: {os.environ.get('PYTHONHASHSEED', 'Not set')}\")\n",
    "    print(f\"TF_DETERMINISTIC_OPS: {os.environ.get('TF_DETERMINISTIC_OPS', 'Not set')}\")\n",
    "    print(f\"TF_CUDNN_DETERMINISTIC: {os.environ.get('TF_CUDNN_DETERMINISTIC', 'Not set')}\")\n",
    "    print(\"=====================================\")\n",
    "\n",
    "# Uncomment the line below to verify seed settings\n",
    "# verify_seed_settings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data \n",
    "stock_prices_df, stock_shares_amount_df, mkt_cap_df, spx_index, removed_companies = get_data()\n",
    "top_100_mkt_cap_df, top_100_mkt_cap_prices_df=get_top_mkt_cap_stocks(stock_prices_df=stock_prices_df, \n",
    "stock_mkt_cap_df=mkt_cap_df)\n",
    "\n",
    "\n",
    "# Calculate daily returns\n",
    "stocks_returns    = np.log(top_100_mkt_cap_prices_df / top_100_mkt_cap_prices_df.shift(1))\n",
    "sp500_idx_returns =  np.log(spx_index / spx_index.shift(1))\n",
    "\n",
    "# Rolling volatility\n",
    "window_size = 252*2 # 2 years\n",
    "Sigma_df= stocks_returns.rolling(window=window_size).cov(pairwise=True)\n",
    "Sigma_df = Sigma_df.dropna()\n",
    "\n",
    "# Get the first date in the cleaned DataFrame\n",
    "START_DATE = Sigma_df.index.get_level_values(0)[0]\n",
    "\n",
    "# Filter dataframes to start from START_DATE\n",
    "top_100_mkt_cap_df = top_100_mkt_cap_df.loc[START_DATE:]\n",
    "stocks_returns     = stocks_returns.loc[START_DATE:]\n",
    "stock_prices_df    = stock_prices_df.loc[START_DATE:]\n",
    "\n",
    "assets = stocks_returns.columns\n",
    "dates  = stocks_returns.index\n",
    "\n",
    "n = len(assets)\n",
    "T = len(dates)\n",
    "Sigma_t = np.empty((T, n, n))\n",
    "\n",
    "# Fill array with each rolling covariance matrix\n",
    "for i, t in enumerate(dates):\n",
    "    Sigma = Sigma_df.loc[t].reindex(index=assets, columns=assets).values\n",
    "    Sigma_t[i] = Sigma\n",
    "\n",
    "\n",
    "# Mkt Weights\n",
    "mkt_portfolio_weights = top_100_mkt_cap_df.div(top_100_mkt_cap_df.sum(axis=1),axis=0)\n",
    "\n",
    "#Retorno do portfólio de mercado\n",
    "mkt_return = (mkt_portfolio_weights.shift(1) * stocks_returns).sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing tau_t calculation\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'T' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 28\u001b[39m\n\u001b[32m     25\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33minitializing tau_t calculation\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     26\u001b[39m start_time = time.time()\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m tau_t = np.empty((\u001b[43mT\u001b[49m, n, n))\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(T):\n\u001b[32m     30\u001b[39m     tau_t[t] = compute_relative_covariance(Sigma_t[t], mkt_portfolio_weights.iloc[t].values)\n",
      "\u001b[31mNameError\u001b[39m: name 'T' is not defined"
     ]
    }
   ],
   "source": [
    "def compute_relative_covariance(sigma: np.ndarray, pi: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Calcula a matriz de covariância relativa τ_ij^π(t) com base na covariância σ_ij(t)\n",
    "    e no vetor de pesos do portfólio π(t).\n",
    "    \n",
    "    Args:\n",
    "        sigma (np.ndarray): Matriz de covariância dos ativos (n x n).\n",
    "        pi (np.ndarray): Vetor de pesos do portfólio (n,).\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Matriz de covariância relativa τ^π (n x n).\n",
    "    \"\"\"\n",
    "    # Covariância ativo i com portfólio: sigma_iπ = sigma @ pi\n",
    "    sigma_i_pi = sigma @ pi        # (n,)\n",
    "    sigma_pi_pi = pi.T @ sigma @ pi  # escalar\n",
    "\n",
    "    # Matriz τ_ij^π = σ_ij - σ_iπ - σ_jπ + σ_ππ\n",
    "    n = len(pi)\n",
    "    tau = np.zeros((n, n))\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            tau[i, j] = sigma[i, j] - sigma_i_pi[i] - sigma_i_pi[j] + sigma_pi_pi\n",
    "    return tau\n",
    "\n",
    "print(\"initializing tau_t calculation\")\n",
    "start_time = time.time()\n",
    "\n",
    "tau_t = np.empty((T, n, n))\n",
    "for t in range(T):\n",
    "    tau_t[t] = compute_relative_covariance(Sigma_t[t], mkt_portfolio_weights.iloc[t].values)\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_seconds = end_time - start_time\n",
    "print(f\"Elapsed time: {elapsed_seconds:.2f} seconds\")\n",
    "\n",
    "\n",
    "# Copying data generated previously \n",
    "mu_t_df = mkt_portfolio_weights.copy()\n",
    "R_t_df =  stocks_returns.copy()\n",
    " \n",
    "mu_tf                   = tf.convert_to_tensor(mu_t_df.values, dtype=tf.float32) # Pesos de mercado\n",
    "mkt_ret_tf              = tf.convert_to_tensor(mkt_return.values, dtype=tf.float32)              # Retorno do portfólio de mercado\n",
    "R_t_tf                  = tf.convert_to_tensor(R_t_df.values, dtype=tf.float32)\n",
    "tau_t_tf                = tf.convert_to_tensor(tau_t, dtype=tf.float32)\n",
    "\n",
    "\n",
    "indices = np.arange(len(mu_tf))\n",
    "train_idx, test_idx = train_test_split(indices, test_size=0.1, shuffle=False, random_state=SEED)\n",
    "\n",
    "# Split datasets\n",
    "\n",
    "#Treino\n",
    "mu_train            = tf.gather(mu_tf, train_idx)\n",
    "mkt_ret_train       = tf.gather(mkt_ret_tf, train_idx)\n",
    "stock_returns_train = tf.gather(R_t_tf, train_idx)\n",
    "tau_train           = tf.gather(tau_t_tf, train_idx)\n",
    "\n",
    "# Teste\n",
    "mu_test            = tf.gather(mu_tf, test_idx)\n",
    "mkt_ret_test       = tf.gather(mkt_ret_tf, test_idx)\n",
    "stock_returns_test = tf.gather(R_t_tf,  test_idx)\n",
    "tau_test           = tf.gather(tau_t_tf, test_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PINN(tf.keras.Model):\n",
    "    def __init__(self, input_dim=200):\n",
    "        \"\"\"\n",
    "        Initializes the PINN model with an architecture optimized to learn\n",
    "        smooth (C^2).\n",
    "\n",
    "        Args:\n",
    "            input_dim (int): The dimensionality of the network input (default: 200).\n",
    "        \"\"\"\n",
    "        super(PINN, self).__init__()\n",
    "\n",
    "        self.lambda1 = tf.Variable(initial_value=1.0, trainable=False, dtype=tf.float32)\n",
    "        self.lambda2 = tf.Variable(initial_value=1.0, trainable=False, dtype=tf.float32)\n",
    "        \n",
    "        initializer = tf.keras.initializers.GlorotNormal(seed=SEED)\n",
    "        l2_regularizer = tf.keras.regularizers.l2(0.001)\n",
    "\n",
    "        self.hidden_layers = tf.keras.Sequential([\n",
    "            tf.keras.layers.InputLayer(input_shape=(input_dim,)),\n",
    "            tf.keras.layers.Dense(100, activation='swish', kernel_initializer=initializer),\n",
    "            tf.keras.layers.Dense(100, activation='swish', kernel_initializer=initializer, kernel_regularizer=l2_regularizer),\n",
    "            tf.keras.layers.Dense(100, activation='swish', kernel_initializer=initializer, kernel_regularizer=l2_regularizer),\n",
    "            \n",
    "        ])\n",
    "        self.output_layer = tf.keras.layers.Dense(1, activation='softplus')\n",
    "\n",
    "    @tf.function\n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "        Executa a passagem para a frente (forward pass).\n",
    "        A lógica de separação e concatenação foi removida por ser redundante.\n",
    "        \"\"\"\n",
    "        z = self.hidden_layers(inputs)\n",
    "        return self.output_layer(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################\n",
    "## Model Compilation ##\n",
    "###########################################################\n",
    "\n",
    "model =  PINN(input_dim=100)\n",
    "\n",
    "steps_bounds = [1000]\n",
    "lr_values = [1e-2, 1e-3]\n",
    "lr_schedule = tf.keras.optimizers.schedules.PiecewiseConstantDecay(steps_bounds, lr_values)\n",
    "optimizer = tf.keras.optimizers.Adam(lr_schedule, beta_1=0.9, beta_2=0.95)\n",
    "optimizer_self_adp = tf.keras.optimizers.Adam(1e2, beta_1=0.9, beta_2=0.99)\n",
    "\n",
    "\n",
    "#######################################\n",
    "## Objects to store training metrics ##\n",
    "#######################################\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "grad_norms_per_epoch = {}\n",
    "mkt_weights_per_epoch = {}\n",
    "drift_per_epoch = {}\n",
    "\n",
    "avg_epoch_loss_vect = []\n",
    "grad_norms_batch_vect     = []\n",
    "mkt_weights_per_batch_vect = []\n",
    "drift_per_batch_vect = []\n",
    "\n",
    "###########################################\n",
    "## Initialize self adaptative parameters ##\n",
    "###########################################\n",
    "\n",
    "self_adp_lambda1 = tf.cast(tf.zeros([1,1]), dtype=tf.float64)\n",
    "self_ada_lambda2 = tf.cast(tf.zeros([1,1]), dtype=tf.float64)\n",
    "\n",
    "self_adp_lambda1 = tf.Variable(self_adp_lambda1)\n",
    "self_ada_lambda2 = tf.Variable(self_ada_lambda2)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    poch_loss = 0\n",
    "    num_batches = 0\n",
    "\n",
    "    ################\n",
    "    ## Train Step ##\n",
    "    ################\n",
    "\n",
    "    total_loss, equation_error, positivity_penalty, g_t, pi_t, gradient_self_adp = train_step(model, optimizer, mu_train, stock_returns_train, mkt_ret_train, tau_train)\n",
    "\n",
    "\n",
    "    #####################################\n",
    "    ## Update self-adaptive parameters ##\n",
    "    #####################################\n",
    "\n",
    "    optimizer_self_adp.apply_gradients(zip(gradient_self_adp, [self_adp_lambda1, self_ada_lambda2]))\n",
    "    self_adp_lambda1.assign(tf.math.softplus(self_adp_lambda1))\n",
    "    self_ada_lambda2.assign(tf.math.softplus(self_ada_lambda2))\n",
    "\n",
    "\n",
    "\n",
    "    if epoch % 1 == 0:\n",
    "        print('\\n',\n",
    "                'Testing error for Epoch {0}:  total_loss --> {1}, pde_loss -->{2}, bound_loss -->{3}, error --> {4}'.format(\n",
    "                    epoch, total_loss.numpy(), equation_error.numpy(), positivity_penalty.numpy()))\n",
    "        #print('*' * 100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "    @tf.function\n",
    "    def loss_function(mu, G_pred, grad_log_G_pred, HESS_G_pred, ret, ret_mkt, tau, adp_lambda1, ada_lambda2):\n",
    "        \"\"\"\n",
    "        Custom loss function for the PINN.\n",
    "\n",
    "        Args:\n",
    "            mu (tf.Tensor): Market weights.\n",
    "            G_pred (tf.Tensor): Value of G predicted by the network.\n",
    "            grad_log_G_pred (tf.Tensor): Gradient of log of G predicted by the network.\n",
    "            HESS_G_pred (tf.Tensor): Hessian of G predicted by the network.\n",
    "            ret (tf.Tensor): Stock returns.\n",
    "            ret_mkt (tf.Tensor): The market return.\n",
    "            tau (tf.Tensor): Relative covariance matrix.\n",
    "\n",
    "        Returns:\n",
    "            _type_: Loss values\n",
    "        \"\"\"\n",
    "        ###################################################################################\n",
    "        # # Functionally Generated Portfolio - Generated by the Neural Network function # #\n",
    "        ###################################################################################\n",
    "\n",
    "        inner_prod = tf.reduce_sum(mu * grad_log_G_pred, axis=1)\n",
    "        pi_t = ((grad_log_G_pred + 1) - tf.expand_dims(inner_prod, axis=1))*mu\n",
    "\n",
    "        # Shift weights to consider trade date at the end of the day\n",
    "        pi_t_shifted = tf.concat([pi_t[:1], pi_t[:-1]], axis=0)\n",
    "        # Portfolio return\n",
    "        port_ret = tf.reduce_sum(pi_t_shifted * ret, axis=1)\n",
    "\n",
    "        #########################\n",
    "        # # Drift Calculation # #\n",
    "        #########################\n",
    "        T = tf.shape(pi_t)[0]\n",
    "\n",
    "        mu_i = tf.expand_dims(mu, axis=2)     # (T, n, 1)\n",
    "        mu_j = tf.expand_dims(mu, axis=1)     # (T, 1, n)\n",
    "\n",
    "        # internal product μ_i μ_j: shape (T, n, n)\n",
    "        mu_outer = mu_i * mu_j\n",
    "\n",
    "        # elementwise: H * μ_i * μ_j * τ\n",
    "        elementwise = HESS_G_pred * mu_outer * tau  # shape (T, n, n)\n",
    "\n",
    "        # sum over i and j (last two dimensions)\n",
    "        summed = tf.reduce_sum(elementwise, axis=[1, 2])  # shape (T,)\n",
    "        dg_t = -0.5 * summed / G_pred\n",
    "\n",
    "        # #########################\n",
    "        # # # Drift integration # #\n",
    "        # #########################\n",
    "        base = tf.range(T)\n",
    "\n",
    "\n",
    "        weights = tf.ones_like(dg_t, dtype=dg_t.dtype)\n",
    "        if T % 2 == 1:  \n",
    "            weights = tf.where(\n",
    "                (base % 2 == 1) & (base != 0) & (base != T-1),\n",
    "                tf.constant(4.0, dtype=dg_t.dtype),\n",
    "                weights\n",
    "            )\n",
    "            weights = tf.where(\n",
    "                (base % 2 == 0) & (base != 0) & (base != T-1),\n",
    "                tf.constant(2.0, dtype=dg_t.dtype),\n",
    "                weights\n",
    "            )\n",
    "            weights *= (1.0 / 3.0)\n",
    "        else:  # Trapezoidal rule\n",
    "            weights = tf.where(\n",
    "                (base == 0) | (base == T-1),\n",
    "                tf.constant(0.5, dtype=dg_t.dtype),\n",
    "                tf.constant(1.0, dtype=dg_t.dtype)\n",
    "            )\n",
    "        g_t = tf.reduce_sum(weights * dg_t)\n",
    "\n",
    "        # #######################\n",
    "        # # # Master Eq Error # #\n",
    "        # #######################\n",
    "\n",
    "        eps = 1e-6\n",
    "        G0 = tf.clip_by_value(G_pred[0], clip_value_min=eps, clip_value_max=1e6)\n",
    "        GT = tf.clip_by_value(G_pred[-1], clip_value_min=eps, clip_value_max=1e6)\n",
    "        right_hand_side = tf.math.log(G0[0]) - tf.math.log(GT[0]) + g_t\n",
    "\n",
    "        # Cumulative log return\n",
    "        port_cumulative_return = tf.math.exp(tf.reduce_sum(port_ret))\n",
    "        mkt_cumulative_return = tf.math.exp(tf.reduce_sum(ret_mkt))\n",
    "        left_hand_side = tf.math.log(port_cumulative_return) - tf.math.log(mkt_cumulative_return)\n",
    "\n",
    "        equation_error = tf.square(left_hand_side - right_hand_side)  # Smooth, differentiable\n",
    "        positivity_penalty = tf.square(tf.nn.relu(-g_t)) # Penalize negative drift\n",
    "\n",
    "        total_loss = adp_lambda1 * equation_error + ada_lambda2 * positivity_penalty\n",
    "\n",
    "        return total_loss, equation_error, positivity_penalty, g_t, pi_t\n",
    "\n",
    "    @tf.function\n",
    "    def train_step(model, optimizer, x, ret, ret_mkt, tau):\n",
    "        with tf.GradientTape(persistent=True) as tape2:\n",
    "            with tf.GradientTape() as tape1:\n",
    "                G_pred = model(x)\n",
    "                log_G_pred = tf.math.log(G_pred + 1e-7)\n",
    "                grad_log_G = tape1.gradient(log_G_pred, x)\n",
    "                grad_G       = tape1.gradient(G_pred, x) \n",
    "\n",
    "            H_G = tape2.batch_jacobian(grad_G, x)  # Hessian of G(x)\n",
    "            total_loss, equation_error, positivity_penalty, g_t, pi_t = loss_function(mu=x, G_pred= G_pred, grad_log_G_pred= grad_log_G, HESS_G_pred= H_G, ret= ret, ret_mkt= ret_mkt, tau= tau, adp_lambda1= self_adp_lambda1, ada_lambda2= self_ada_lambda2)\n",
    "\n",
    "        gradient_nn_wt = tape1.gradient(total_loss, model.trainable_variables)\n",
    "        (gradient_self_adp) = tape1.gradient(total_loss, [self_adp_lambda1, self_ada_lambda2])\n",
    "\n",
    "        optimizer.apply_gradients(zip(gradient_nn_wt, model.trainable_variables))\n",
    "\n",
    "        return total_loss, equation_error, positivity_penalty, g_t, pi_t, gradient_self_adp\n",
    "\n",
    "    #######################\n",
    "    ## Model Compilation ##\n",
    "    #######################\n",
    "\n",
    "    model =  PINN(input_dim=100)\n",
    "\n",
    "    steps_bounds = [1000]\n",
    "    lr_values = [1e-2, 1e-3]\n",
    "    lr_schedule = tf.keras.optimizers.schedules.PiecewiseConstantDecay(steps_bounds, lr_values)\n",
    "    optimizer = tf.keras.optimizers.Adam(lr_schedule, beta_1=0.9, beta_2=0.95)\n",
    "    optimizer_self_adp = tf.keras.optimizers.Adam(1e2, beta_1=0.9, beta_2=0.99)\n",
    "\n",
    "\n",
    "    #######################################\n",
    "    ## Objects to store training metrics ##\n",
    "    #######################################\n",
    "\n",
    "    epochs = 10\n",
    "\n",
    "    grad_norms_per_epoch = {}\n",
    "    mkt_weights_per_epoch = {}\n",
    "    drift_per_epoch = {}\n",
    "\n",
    "    avg_epoch_loss_vect = []\n",
    "    grad_norms_batch_vect     = []\n",
    "    mkt_weights_per_batch_vect = []\n",
    "    drift_per_batch_vect = []\n",
    "\n",
    "    ###########################################\n",
    "    ## Initialize self adaptative parameters ##\n",
    "    ###########################################\n",
    "\n",
    "    self_adp_lambda1 = tf.cast(tf.zeros([1,1]), dtype=tf.float64)\n",
    "    self_ada_lambda2 = tf.cast(tf.zeros([1,1]), dtype=tf.float64)\n",
    "\n",
    "    self_adp_lambda1 = tf.Variable(self_adp_lambda1)\n",
    "    self_ada_lambda2 = tf.Variable(self_ada_lambda2)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        poch_loss = 0\n",
    "        num_batches = 0\n",
    "\n",
    "        ################\n",
    "        ## Train Step ##\n",
    "        ################\n",
    "\n",
    "        total_loss, equation_error, positivity_penalty, g_t, pi_t, gradient_self_adp = train_step(model, optimizer, mu_train, stock_returns_train, mkt_ret_train, tau_train)\n",
    "\n",
    "\n",
    "        #####################################\n",
    "        ## Update self-adaptive parameters ##\n",
    "        #####################################\n",
    "\n",
    "        optimizer_self_adp.apply_gradients(zip(gradient_self_adp, [self_adp_lambda1, self_ada_lambda2]))\n",
    "        self_adp_lambda1.assign(tf.math.softplus(self_adp_lambda1))\n",
    "        self_ada_lambda2.assign(tf.math.softplus(self_ada_lambda2))\n",
    "\n",
    "\n",
    "\n",
    "        if epoch % 1 == 0:\n",
    "            print('\\n',\n",
    "                    'Testing error for Epoch {0}:  total_loss --> {1}, pde_loss -->{2}, bound_loss -->{3}, error --> {4}'.format(\n",
    "                        epoch, total_loss.numpy(), equation_error.numpy(), positivity_penalty.numpy()))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
